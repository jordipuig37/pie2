---
title: "CheatSheet"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

# Models lineals

### Prèvia
LLibreries que acostumem a incloure encara que no sabem exactament què fan.
```{r}
library(emmeans)
library(car)
library(RcmdrMisc)
library(tables)
```

### Basics
Carregar les dades del fitxer data.csv i plantejar un model.

```{r}
setwd('.')
dades<-read.csv2("./data.csv")
# dades<-read.csv2("~/uni/2n/prob2/exercicis r/comrect.csv")
# Assegurar-se que les columnes que han de ser factors ho son
dades$Any<-as.factor(dades$Any)
dades$M<-as.factor(dades$M)

# per comprovar-ho:
is.factor(dades$Any)
# plantejar un model
model = lm(formula=Y~X, data=dades)
```

Exemples de fórmules per diferents models:

```{r}
f0 = Y ~ 1 # model null
f1 = Y ~ X # regresió simple
f2 = Y ~ X1 + X2 # model aditiu
f22= Y ~ X + M # model aditiu però una variable és un factor en realitat
f3 = Y ~ X * M # regresió simple amb un factor o model factorial
```


#### Per veure ràpidament les dades:  
__Funció scatterplot per les rectes de regressió__\
*scatterplot(formula(factorial), data)* ploteja la variable resposta vs la variable en una recta per cada factor. Si la fórmula és d'un model aditiu es veu només una recta. Aquí és on hem d'observar linealitat per complir les hipòtesis dels models lineals.


```{r}
scatterplot(Y~X*M,smooth=F, boxplot = F,data=dades)
```


__Les mitjanes per diferents factors__\
*with(data, plotMeans(response, factor1, factor2))* fa la mitjana per cada combinació dels dos factors 1 i 2.

```{r}
with(dades,plotMeans(response=Y,factor1=Any,factor2=M,error.bars="none",level=0.95))
```


__EMMeans__\
EMM (estimated marginal means) calcula les mitjanes marginals per alguns factors especificats a *specs*
Podem veure quins factors són estadísticament diferents (amb un nivell de significació donat) visualment amb pwpp(em) si no estan units per la línia. Tenim funcions interessants com pairs(em) que fan un anàlisis de *emm* per parells. Calcula .

```{r}
dades_emm <- read.csv2("./gmd.csv")
m <- lm(GMD~DOSI,dades_emm)

(emm<-emmeans(m,~DOSI))

pairs(emm)
CLD(emm)
```

La funció *pwpp()* amb el paràmetre *adjust="tukey"* ens fa una gràfica que relaciona cada parell de factors i diu si són estadísticament diferents per un nivell de significació donat en l'eix x.

```{r}
pwpp(emm, adjust="tukey")
```

En aquest cas diríem que donat un nivell de significació del 0.01 són diferents si estan units, per tant D15 i D08, D00 i D15, D00 amb tots menys amb D08...

### Comprovacions

#### Test Anova i Omnibus
Per fer les taules anova utilitzem la funció anova(model). Tenim tipus I, II, III i la més comú i recomenable és el tipus II (Anova()). Ens indica la significació de cada paràmetre (si cada paràmetre sol és igual a zero o no) i l'estadístic de contrast (F value)

```{r}
mod = lm(Y~X*M, dades)
m0d = lm(Y~1, dades)

Anova(mod)
anova(mod,m0d)
```

#### Summary

La funció summary() del model ens dona un resum. Ens dona els paràmetres del model estimats, l'error estàndard, i el t-valor i la significació del paràmetre.

Per la variancia de l'error afegim $sigma^2

```{r}
summary(mod)$sigma^2
summary(mod)
```


#### Errors

__Errors vs Y__\
Els errors els podem veure amb la funció plot i el parametre *which* = 1.
La funció plot té més funcionalitats si el primer paràmetre és el model lineal i *which* = [1,6]:

1. A plot of residuals against fitted values\
2. A normal Q-Q plot\
3. A Scale-Location plot of sqrt(| residuals |) against fitted values\
4. A plot of Cook's distances versus row labels\
5. A plot of residuals against leverages\
6. A plot of Cook's distances against leverage/(1-leverage)\

Ara veiem el plot dels residus. Recorda que s'ha de veure una distribució uniforme sobre el pla. Si es veu alguna forma d'embut o parabòlica cal aplicar transformacions. Aquesta distribució uniforme verifica la hipòtesis de l'independència dels errors.

```{r out.width = '50%'}
dd <- read.csv2("./col.csv")
mod<-lm(C~P,dd)
# plot(mod,which=1)
plot(predict(mod),resid(mod),main="Residuals vs Predits")
# línia al 0 per veure el centre
abline(h=0,lty=2)
```



__Errors studentizats__
Aqui cal veure que no hi ha valors per sobre del 3 o més avall del -3, y a més que es pot observar normalitat del valors.
```{r}
plot(rstudent(mod),main="Residuals studentitzats")
abline(h=c(-3,-2,0,2,3),lty=2)
```


__Valors influents__\
Els DFFITS són una mesura similar a la distància de Cook. Ens indica si hi ha algun residu molt lluny.

```{r}
p<-2
n<-dim(dd)[1]
plot(dffits(mod),main="DFFITS")
# Les línies que posem son als valors {-3, -2, 0, 2, 3} però multiplicades per un factor de squrt(p/n)
abline(h=c(-3,-2,0,2,3)*sqrt(p/n),lty=2)
```


__QQ-plot dels errors__\
Per fer la segona comprovació dels errors fem l'anomenat qq-plot dels errors. Aquí hem  d'observar linealitat.

```{r}
plot(mod, which=2)
```

### Tests d'hipòtesis
per plantejar un test d'hipòtesis de l'estil:\
$H_{0}:E\left[C|P=60,E=15,H=150\right]=200$\
$H_{1}:E\left[C|P=60,E=15,H=150\right]\neq200$.
```{r}
dd$EP<-dd$P-(dd$H/2-10)
modE2v<-lm(C~EP+E,dd)
P = 60
E = 15
H = 150
lht(modE2v,c(1,P-(H/2-10),E),200)
```

## Transformacions
Quan els models no compleixen les condicions, cal fer transformacions de diferents tipus per ajustar les prediccions. A vegades cal provar diferents transformacions per saber quina és la bona, per comprovar-ho anem fent les gràfiques dels residus i les de les bandes de predicció. Posarem l'exemple del ficus perquè és més veganfriendly.

Primer carreguem les dades i fem una mica de descriptiva.
```{r}
ddt <- read.csv2("./ficus.csv")
# una taula que diu coses
cv<-function(x) {sd(x)/mean(x)}
di<-function(x) {var(x)/mean(x)}
t<-tabular((DIES=as.factor(DIES))~H*((n.dades=1)+(mitjana=mean)+(desv.tipus=sd)+(coef.variació=cv)+(index.disp.=di)),ddt)

scatterplot(H~DIES, regLine=F, smooth=F, boxplots=F,pch=3, data=ddt)
lines(rowLabels(t),t[,2])
```
#### Regressió simple

Bueno aquest model (regressió lineal) és per veure que als residus veiem formes xungues que no hauríem de veure. Posem les tres gràfiques: bandes de predicció, residus vs predits i residus studentitzats.

```{r, fig.show="as.is"}
ma<-lm(H~DIES,ddt)
dies=1:150


# Bandes de predicció
pra<-predict(ma,data.frame(DIES=dies),interval="prediction")
plot(ddt$DIES,ddt$H,pch=3,xlim=c(0,150),xlab="dies",ylab="H",ylim=c(min(ddt$H,pra),max(ddt$H,pra)),main="Model A")
lines(dies,pra[,"fit"],col="blue")
lines(dies,pra[,"lwr"],col="red")
lines(dies,pra[,"upr"],col="red")

# residus
plot(predict(ma),resid(ma))
abline(h=0,lt=2)

plot(rstudent(ma))
abline(h=c(-3,-2,0,2,3),lt=2)

```

#### Paràbola

Ara considerarem com a funció de regressió la paràbola de H respecte DIES. Cal afegir una columna que són les observacions al quadrat (en aquest cas DIES). Si es vol augmentar el grau del polinomi cal afegir columnes d'acord amb el grau.

```{r}
# veiem com plantegem el model diferent (creem una columna nova a ddt):
ddt$DIES2 <- ddt$DIES^2
mb<-lm(H~DIES+DIES2,ddt)

# Bandes de predicció
prb<-predict(mb,data.frame(DIES=dies,DIES2=dies^2),interval="prediction")
plot(ddt$DIES,ddt$H,pch=3,xlim=c(0,150),xlab="dies",ylab="H",ylim=c(min(ddt$H,prb),max(ddt$H,prb)),main="Model B")
lines(dies,prb[,"fit"],col="blue")
lines(dies,prb[,"lwr"],col="red")
lines(dies,prb[,"upr"],col="red")


# residus
plot(predict(mb),resid(mb))
abline(h=0,lt=2)

plot(rstudent(mb))
abline(h=c(-3,-2,0,2,3),lt=2)
```

#### Logaritme
Ara la transformació és amb el logaritme. Tenim que:
$E\left[log\left(H\right)|DIES\right]=\alpha+\beta\cdot DIES$ i $Var\left(log\left(H\right)|DIES\right)=\sigma^{2}.$\
L'interessant és que la transformació log homogeneitza les variàncies quan Var∝μ2 encara que l’important en aquest cas és que linealitza la funció eα+β⋅DIES.
```{r}
# model log(H)
mc<-lm(log(H)~DIES,ddt)
# també tenim transformacions del tipus: (producció de llet)
# ml<-lm(log(PROD)~DIES+log(DIES),dd)

# bandes de predicció
prc<-predict(mc,data.frame(DIES=dies),interval="prediction")
plot(ddt$DIES,ddt$H,pch=3,xlim=c(0,150),xlab="dies",ylab="H",ylim=c(min(ddt$H,exp(prc)),max(ddt$H,exp(prc))),main="Model C")
lines(dies,exp(prc[,"fit"]),col="blue")
lines(dies,exp(prc[,"lwr"]),col="red")
lines(dies,exp(prc[,"upr"]),col="red")

#residus
plot(predict(mc),resid(mc))
abline(h=0,lt=2)

plot(rstudent(mc))
abline(h=c(-3,-2,0,2,3),lt=2)
```

#### Arrel quadrada
La transformació √ homogeneitza les variàncies quan Var∝μ.

```{r}
# veiem la transformació sqrt(H)
md<-lm(sqrt(H)~DIES,ddt)

# bandes
prd<-predict(md,data.frame(DIES=dies),interval="prediction")
plot(ddt$DIES,ddt$H,pch=3,xlim=c(0,150),xlab="dies",ylab="H",ylim=c(min(ddt$H,prd^2),max(ddt$H,prd^2)),main="Model D")
lines(dies,prd[,"fit"]^2,col="blue")
lines(dies,prd[,"lwr"]^2,col="red")
lines(dies,prd[,"upr"]^2,col="red")

# residus
plot(predict(md),resid(md))
abline(h=0,lt=2)

plot(rstudent(md))
abline(h=c(-3,-2,0,2,3),lt=2)
```

### Test Levene
Serveix per comprovar la hipòtesis d'Homocedasticitat. Per acceptar l'hipòtesis nul·la hem de veure un p-valor menor a 0.05. A més cal posar Factors o una cosa així.

```{r}
ddt$FDIES<-as.factor(ddt$DIES)
leveneTest(resid(md),ddt$FDIES)
```

### Comparació

Podem fer una taula per comparar diferents models; per exemple, dels que han sortit comparem la logLikliehood, AIC, BIC (com més petits millor) i l'R^2 ajustat:

```{r}
rbind(
  "logLik"=c("Model A"=logLik(ma),"Model B"=logLik(mb),"Model C"=logLik(mc),"Model D"=logLik(md)),
  "AIC"=c(AIC(ma),AIC(mb),AIC(mc),AIC(md)),
  "BIC"=c(BIC(ma),BIC(mb),BIC(mc),BIC(md)),
  "R2"=c(summary(ma)$adj.r.squared,summary(mb)$adj.r.squared,summary(mc)$adj.r.squared,summary(md)$adj.r.squared))

```


# GLM

Aqui es donde comienza la fiesta. Primero de todo, las familias:

a. Normal
b. Binomial
c. Poisson
d. Gamma
e. Inversa Gaussiana
f. Otras

Si te olvidas de como tienes que escribirlas solo ve a `family`.
Segundo, las funciones link se especifican justo al lado y entre paréntesis de la familia. Ej.: `family=binomial(link=probit)`. Para hacer un ejemplo completo usaré uno de los últimos ejercicios:
```{r}
ddglm <- data.frame(time=c(16,16,16,24,24,24),
                  dose=c(0, 0.45, 0.75, 0, 0.45, 0.75),
                  tumor=c(1,3,7, 20,98,118),
                  total=c(205,304,193, 762,888,587))
ddglm$time <- as.factor(ddglm$time)

m <- glm(tumor/total~time*dose, data=ddglm, family=binomial, weights=total)
anova(m, test="Chisq")
summary(m)
```

En general se suele usar el test Chi cuadrado para hacer el anova, aunque también puede aparecer el F de fisher, depende de la familia, por suerte R te suele avisar si lo pones mal. Por lo demás, el análisis es como en los modelos lineales.

#### Estadístics
Aquí tenim els estadístics de Pearson, paràmetres de dispersió de Pearson i de deviance, i la variància $Var\left(Y|\left\{ a,b,c\right\} \right)$.

```{r}
c(Pearson=Pearson<-sum(resid(m,ty="pearson")^2))
c(dispP=dispP<-Pearson/m$df.residual)
c(Deviance=Deviance<-sum(resid(m,ty="deviance")^2),m$deviance)
c(dispD=dispD<-Deviance/m$df.residual)
```


### Residuos
Hay de dos tipos, devianza y pearson. Se puede utilizar para calcular el parámetro de dispersión que en el caso de la Poisson y la binomial debe ser cercano a 1.
```{r}
resid(m, ty="pearson")
resid(m, ty="deviance")

PRS <- sum(resid(m, ty="pearson"))
c("Par.disp"=PRS/m$df.res, "p-valor"=2*min(pchisq(PRS,m$df.res),1-pchisq(PRS,m$df.res)))
```
En este caso sale infradispersión, si sale sobredispersión entonces hay que preocuparse mal y o bien los datos no siguen la distribución esperada o el modelo está mal. También se pueden hacer los análisis de siempre.
```{r}
plot(m, 1)
plot(rstudent(m))
abline(h=c(-2,0,2), lty=2)
```


### Predicciones

```{r}
Dose <- seq(0, 1, .01)
plot(tumor/total~(dose), data=ddglm)
lines(predict(m, data.frame(time=as.factor(rep(16, 101)), dose=Dose), ty="response")~(Dose), col="red")
lines(predict(m, data.frame(time=as.factor(rep(24, 101)), dose=Dose), ty="response")~(Dose), col="green")
abline(h=c(0,1), col="grey")
```
